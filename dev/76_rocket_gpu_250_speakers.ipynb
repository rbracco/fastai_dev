{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.basics import *\n",
    "from local.data.all import *\n",
    "from local.vision.core import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.audio.core import *\n",
    "from local.audio.augment import *\n",
    "from local.vision.learner import *\n",
    "from local.vision.models.xresnet import *\n",
    "from local.metrics import *\n",
    "from local.callback.schedule import *\n",
    "import torchaudio\n",
    "from fastprogress import progress_bar as pb\n",
    "from numba import njit, prange\n",
    "from rocket import ROCKET, ROCKETMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PROCESSED =  PATH_ORIGINAL/\"250-speakers/.processed\"\n",
    "x = AudioGetter(\"\", recurse=True, folders=None)\n",
    "files_250  = x(PATH_PROCESSED)\n",
    "oa = OpenAudio(files_250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#44655) [/home/jupyter/.fastai/data/250_speakers/250-speakers/.processed/id09028/_KymcHdEW0U/00034.wav,/home/jupyter/.fastai/data/250_speakers/250-speakers/.processed/id09028/2HgpwyiMUEE/00002.wav,/home/jupyter/.fastai/data/250_speakers/250-speakers/.processed/id09028/11xxoaj4aEA/00001.wav,/home/jupyter/.fastai/data/250_speakers/250-speakers/.processed/id09028/gC8wHtwhnZw/00036.wav,/home/jupyter/.fastai/data/250_speakers/250-speakers/.processed/id09028/gC8wHtwhnZw/00038.wav,/home/jupyter/.fastai/data/250_speakers/250-speakers/.processed/id09028/gC8wHtwhnZw/00037.wav,/home/jupyter/.fastai/data/250_speakers/250-speakers/.processed/id09028/DwK2JNH10zE/00013.wav,/home/jupyter/.fastai/data/250_speakers/250-speakers/.processed/id09028/DwK2JNH10zE/00012.wav,/home/jupyter/.fastai/data/250_speakers/250-speakers/.processed/id09028/DwK2JNH10zE/00011.wav,/home/jupyter/.fastai/data/250_speakers/250-speakers/.processed/id09028/4f8IoTgW8z0/00005.wav...]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AudioBlock(cls=AudioItem): return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(dbunch):\n",
    "    train_dl, valid_dl = dbunch\n",
    "    sigs_train, labels_train = [],[]\n",
    "    for batch in pb(train_dl):\n",
    "        sigs_train.append(batch[0])\n",
    "        labels_train.append(batch[1])\n",
    "    sigs_valid, labels_valid = [],[]\n",
    "    for batch in pb(valid_dl):\n",
    "        sigs_valid.append(batch[0])\n",
    "        labels_valid.append(batch[1])\n",
    "    sigs_train = torch.cat(sigs_train, axis=0).cpu().numpy()\n",
    "    labels_train = torch.cat(labels_train, axis=0).cpu().numpy()\n",
    "    sigs_valid = torch.cat(sigs_valid, axis=0).cpu().numpy()\n",
    "    labels_valid = torch.cat(labels_valid, axis=0).cpu().numpy()\n",
    "    return sigs_train, labels_train, sigs_valid, labels_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_items=get_audio_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=lambda x: str(x).split('/')[-3][3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Normalize(m, s):\n",
    "    def _inner(ai:AudioItem)->AudioItem:\n",
    "        sig = ai.sig.clone()\n",
    "        norm_sig = (sig - m)/(s + 1e-8)\n",
    "        return AudioItem((norm_sig, ai.sr, ai.path))\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalize(1.5217e-05, 0.0571)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1000' class='' max='1000', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1000/1000 00:05<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_mean = 0\n",
    "total_std = 0\n",
    "total_mean2 = 0\n",
    "total_std2 = 0\n",
    "num_files=len(files_250[0:1000])\n",
    "for i in pb(range(num_files)):\n",
    "    total_mean += oa(i).sig.mean()\n",
    "    total_std  += oa(i).sig.std()\n",
    "    total_mean2 += norm(oa(i)).sig.mean()\n",
    "    total_std2  += norm(oa(i)).sig.std()\n",
    "mn, std = total_mean/num_files, total_std/num_files\n",
    "norm_mn, norm_std = total_mean2/num_files, total_std2/num_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5217e-05)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0571)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-6.4499e-09), tensor(0.9999))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_mn, norm_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "rocket = ROCKET(1, 32000, n_kernels=100, kss=[7, 9, 11], stride=5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSig():\n",
    "    def _inner(ai:AudioItem)->AudioItem:\n",
    "        return ai.sig\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_2000ms = CropSignal(2000)\n",
    "tfms = [crop_2000ms, norm, GetSig(),]\n",
    "gpu_tfms = [Cuda(), rocket]\n",
    "dbunch = auds.databunch(PATH_PROCESSED, item_tfms=tfms,  batch_tfms=gpu_tfms, bs=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='372' class='' max='372', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [372/372 00:31<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='63' class='' max='63', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [63/63 00:08<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 s, sys: 8.68 s, total: 38.3 s\n",
      "Wall time: 39.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, y_train, x_valid, y_valid = extract_data(dbunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = map(lambda x: x.squeeze(1), (x_train, x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35712, 200)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.33 s, sys: 1.49 s, total: 9.82 s\n",
      "Wall time: 2.66 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.21430970775948943"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 7), normalize=True)\n",
    "classifier.fit(x_train, y_train)\n",
    "classifier.score(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocketmax = ROCKETMAX(1, 32000, n_kernels=100, kss=[7, 9, 11], stride=5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_tfms_max = [Cuda(), rocketmax]\n",
    "dbunch = auds.databunch(PATH_PROCESSED, item_tfms=tfms,  batch_tfms=gpu_tfms_max, bs=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='372' class='' max='372', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [372/372 01:22<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='63' class='' max='63', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [63/63 00:19<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 13s, sys: 28.3 s, total: 1min 41s\n",
      "Wall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, y_train, x_valid, y_valid = extract_data(dbunch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35712, 1, 1400)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid = map(lambda x: x.squeeze(1), (x_train, x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.3 s, sys: 4.23 s, total: 32.5 s\n",
      "Wall time: 10.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.34934497816593885"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 7), normalize=True)\n",
    "classifier.fit(x_train, y_train)\n",
    "classifier.score(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
