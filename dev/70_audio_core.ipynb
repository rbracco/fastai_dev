{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp audio.core\n",
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.data.all import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "import torchaudio\n",
    "import torchaudio.transforms as torchaud_tfm\n",
    "import warnings\n",
    "\n",
    "from IPython.display import display, Audio\n",
    "from dataclasses import dataclass, asdict, is_dataclass, make_dataclass\n",
    "from torchaudio.transforms import Spectrogram, AmplitudeToDB, MFCC\n",
    "from librosa.display import specshow, waveplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. [Audio Signals](#Audio-Signals)  \n",
    "    1. [AudioGetter](#AudioGetter)\n",
    "    2. [AudioItem](#AudioItem)\n",
    "    3. [Functions to Wrap TorchAudio](#Create-functions-to-wrap-TorchAudio)\n",
    "2. [Audio Spectrograms](#Audio-Spectrograms)\n",
    "    1. [AudioSpectrogram Class](#AudioSpectrogram-Class)\n",
    "    2. [Spectrogram Generation: AudioToSpec](#Spectrogram-Generation:-AudioToSpec)\n",
    "    3. [MFCC Generation](#MFCC-Generation)\n",
    "3. [Example Pipelines](#Example-Pipelines)\n",
    "    1. [DB MelSpectrogram Pipe](#DB-MelSpectrogram-Pipe-(Standard))\n",
    "    2. [Raw Spectrogram (non-mel, non-db) Pipe](#Raw-Spectrogram-(non-mel,-non-db)-Pipe)\n",
    "    3. [DBScale non-melspectrogram Pipe](#DBScale-non-melspectrogram-Pipe)\n",
    "    4. [From Config Pipe](#Pipe-using-from_cfg-(config))\n",
    "    5. [MFCC Pipe](#MFCC-Pipe)\n",
    "4. [AudioConfig Class](#AudioConfig-Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_all_ = ['AudioGetter', 'get_audio_files', 'AudioItem', 'OpenAudio', 'AudioSpectrogram', 'AudioToSpec',\n",
    "        'SpectrogramConfig', 'AudioConfig', 'audio_extensions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioGetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section regroups the basic types used in vision with the transform that create objects of those types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "audio_extensions = tuple(str.lower(k) for k, v in mimetypes.types_map.items() if v.startswith('audio/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_audio_files(path, recurse=True, folders=None):\n",
    "    \"Get image files in `path` recursively, only in `folders`, if specified.\"\n",
    "    return get_files(path, extensions=audio_extensions, recurse=recurse, folders=folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def AudioGetter(suf='', recurse=True, folders=None):\n",
    "    \"Create `get_image_files` partial function that searches path suffix `suf` and passes along `kwargs`, only in `folders`, if specified.\"\n",
    "    def _inner(o, recurse=recurse, folders=folders): \n",
    "        return get_audio_files(o/suf, recurse, folders)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "URLs.SPEAKERS = 'http://www.openslr.org/resources/45/ST-AEDS-20180100_1-OS'\n",
    "URLs.SPEAKERS250 = 'https://public-datasets.fra1.digitaloceanspaces.com/250-speakers.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Config()['data_path'] / 'ST-AEDS-20180100_1-OS'\n",
    "untar_data(URLs.SPEAKERS, fname=str(p)+'.tar', dest=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_get_func = AudioGetter(\"\", recurse=True, folders=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = audio_get_func(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files will load differently on different machines so we specify examples by name\n",
    "ex_files = [p/f for f in ['m0005_us_m0005_00218.wav', \n",
    "                                'f0003_us_f0003_00279.wav', \n",
    "                                'f0001_us_f0001_00168.wav', \n",
    "                                'f0005_us_f0005_00286.wav',]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioItem(tuple):\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show image using `merge(self._show_args, kwargs)`\"\n",
    "        print(f\"File: {self.path}\")\n",
    "        self.hear()\n",
    "        show_audio_signal(self, ctx=ctx, **kwargs)\n",
    "        plt.show()\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, fn, **kwargs):\n",
    "        sig, sr = torchaudio.load(fn)\n",
    "        return cls((sig, sr, fn))\n",
    "    \n",
    "    sig, sr, path = add_props(lambda i, self: self[i], n=3)\n",
    "    nchannels, nsamples = add_props(lambda i, self: self.sig.shape[i])\n",
    "    @property\n",
    "    def duration(self): return self.nsamples/float(self.sr)\n",
    "    \n",
    "    def hear(self):\n",
    "        display(Audio(self.sig, rate=self.sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_audio_signal(ai, ctx, **kwargs):\n",
    "    if(ai.nchannels > 1):\n",
    "        _,axs = plt.subplots(ai.nchannels, 1, figsize=(6,4*ai.nchannels))\n",
    "        for i,channel in enumerate(ai.sig):\n",
    "            waveplot(channel.numpy(), ai.sr, ax=axs[i], **kwargs)\n",
    "    else:\n",
    "        axs = plt.subplots(ai.nchannels, 1)[1] if ctx is None else ctx \n",
    "        waveplot(ai.sig.squeeze(0).numpy(), ai.sr, ax=axs, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(AudioItem((None, None, ex_files[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0 = AudioItem.create(ex_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0.sig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0.sr, item0.nchannels, item0.nsamples, item0.duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(type(item0.sig), torch.Tensor)\n",
    "test_eq(item0.sr, 16000)\n",
    "test_eq(item0.nchannels, 1)\n",
    "test_eq(item0.nsamples, 58240)\n",
    "test_eq(item0.duration, 3.64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item1 = AudioItem.create(files[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item0.show()\n",
    "item1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 3 equal length portions of 3 different signals so we can stack them\n",
    "#for a fake multichannel example\n",
    "ai0, ai1, ai2 = map(AudioItem.create, ex_files[1:4]);\n",
    "min_samples = min(ai0.nsamples, ai1.nsamples, ai2.nsamples)\n",
    "s0, s1, s2 = map(lambda x: x[:,:min_samples], (ai0.sig, ai1.sig, ai2.sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(s0.shape, s1.shape)\n",
    "test_eq(s1.shape, s2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_multichannel = AudioItem((torch.stack((s0, s1, s2), dim=1).squeeze(0), 16000, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(fake_multichannel.nchannels, 3)\n",
    "test_eq(fake_multichannel.nsamples, 53760)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_multichannel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OpenAudio(Transform):\n",
    "    def __init__(self, items):\n",
    "        self.items = items\n",
    "\n",
    "    def encodes(self, i):\n",
    "        o = self.items[i]\n",
    "        return AudioItem.create(o)\n",
    "    \n",
    "    def decodes(self, i)->Path: \n",
    "        return self.items[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "repr of Transform is:  \n",
    "classname: self.use_as_item {self.encodes} {self.decodes}  \n",
    "encodes and decodes are TypeDispatches whose reprs are str of dict where k/v pair is typename and function that handles that type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa = OpenAudio(files); oa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demonstrate functionality of OpenAudio.encodes, the rest of the nb will\n",
    "#use files that are opened by name for reproducibility/testing\n",
    "oa = OpenAudio(files)\n",
    "item100 = oa.encodes(100)\n",
    "item100.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test open audio on a random set of files\n",
    "for i in range(10):\n",
    "    idx = random.randint(0, len(files))\n",
    "    test_eq_type(oa.encodes(idx), AudioItem.create(files[idx]))\n",
    "    test_eq_type(oa.decodes(idx), files[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(oa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa.encodes(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa.decodes(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa.items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create functions to wrap TorchAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_GenSpec    = torchaudio.transforms.Spectrogram\n",
    "_GenMelSpec = torchaudio.transforms.MelSpectrogram\n",
    "_GenMFCC    = torchaudio.transforms.MFCC\n",
    "_ToDB       = torchaudio.transforms.AmplitudeToDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'><strong>Note: </strong><br> If a function (e.g. specshow) accepts kwargs, this wont pass extra arguments because specshow doesnt accept all kwargs, and will break if you pass in unexpected ones, but we have no way of knowing what functions they delegate to and pulling out the relevant kwargs, so if there is something we know it accepts as a kwarg like \"cmap\" we need to pass it in manually  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'><strong>Note: </strong><br>Add func only works if all args are keyword arguments, doesnt work for unnamed args. Could add in a get usable args that checks if default is inspect._empty. This also needs more tests</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_usable_kwargs` takes a function and a dictionary of kwargs that may or may not be relevant to that function and returns a dictionary of all the default values to that function, updated with the kwargs that can be successfully applied. This is done because, first it allows us to combine multiple functions into a single AudioToSpec Transform but only pass the appropriate kwargs, secondly because it allows us to keep a dictionary of the settings used to create the Spectrogram which is sometimes used in it's display and cropping, and third because it allows us to warn the user when they are passing in improper or unused kwargs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_usable_kwargs(func, kwargs, exclude):\n",
    "    exclude = ifnone(exclude, [])\n",
    "    defaults = {k:v.default for k, v in inspect.signature(func).parameters.items() if k not in exclude}\n",
    "    usable = {k:v for k,v in kwargs.items() if k in defaults}\n",
    "    return {**defaults, **usable}\n",
    "\n",
    "# def add_func(func, kwargs):\n",
    "#     func_args = get_usable_kwargs(func, kwargs, [])\n",
    "#     return func(**func_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'a':1, 'b':2}\n",
    "extra_kwargs = {'z':0, 'a':1, 'b':2, 'c':3}\n",
    "test_eq(get_usable_kwargs(operator.add,       kwargs, []), kwargs)\n",
    "test_eq(get_usable_kwargs(operator.add, extra_kwargs, []), kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'><strong>Note: </strong><br> Overriding getattr to store the settings isnt ideal, but if we dump them all in as attributes by doing `x.__dict__.update(settings)` we then can't easily pass settings when we do a transform and create a new AudioSpectrogram objct. Potential fixes are<br>\n",
    "1. Having both a settings dict and updating the dict with all its attributes (this feels dirty)<br>\n",
    "2. Finding a way to implement deepcopy for AudioSpectrogram so that we can clone it efficiently<br>\n",
    "3. Dumping the spectrogram settings and having a method that collects them so it can be passed to the constructor when we make a new AudioSpectrogram object in a transform<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioSpectrogram Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioSpectrogram(TensorImageBase):\n",
    "    @classmethod\n",
    "    def create(cls, sg, settings=None):\n",
    "        x = cls(sg)\n",
    "        x.settings = settings\n",
    "        return x\n",
    "        \n",
    "    @property\n",
    "    def duration(self):\n",
    "        # spectrograms round up length to fill incomplete columns,\n",
    "        # so we subtract 0.5 to compensate, wont be exact\n",
    "        return (self.hop_length*(self.shape[-1]-0.5))/self.sr\n",
    "    \n",
    "    height, width = add_props(lambda i, self: self.shape[i+1], n=2)\n",
    "    #using the line below instead of above will fix show_batch but break multichannel/delta display\n",
    "    #nchannels, height, width = add_props(lambda i, self: self.shape[i], n=3)\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        if name == \"settings\": return None\n",
    "        if self.settings is not None and name in self.settings: return self.settings[name]\n",
    "        raise AttributeError(f\"{self.__class__.__name__} object has no attribute {name}\")\n",
    "        \n",
    "    def show(self, ctx=None, ax=None, figsize=None, **kwargs):\n",
    "        show_spectrogram(self, ctx=ctx, ax=ax, figsize=figsize,**kwargs)\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'><strong>TO-DO:</strong><br>\n",
    "    1. Get colorbar and axes working for multiplot display <br>\n",
    "    2. Have someone who knows matplotlib better cleanup/refactor<br>\n",
    "    3. Plotting the spectrogram forces it to a uniform size, we may want to display either the\n",
    "    shape of the image, or display it to scale with something like plt.figure(figsize=(sg.width/30, sg.height/30))\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def show_spectrogram(sg, ax, ctx, figsize, **kwargs):\n",
    "    ax = ifnone(ax,ctx)\n",
    "    nchannels = sg.nchannels\n",
    "    r, c = nchannels, sg.data.shape[0]//nchannels\n",
    "    proper_kwargs = get_usable_kwargs(specshow, sg.settings, exclude=[\"ax\", \"kwargs\", \"data\",])\n",
    "    fname = str(sg.path).split('/')[-1] if sg.path is not None else \"Unknown File\"\n",
    "    if (r == 1 and c == 1):\n",
    "        _show_spectrogram(sg, ax, proper_kwargs, **kwargs)\n",
    "        plt.title(f\"{fname}: Channel 0 Image 0\")\n",
    "    else:\n",
    "        if figsize is None: figsize = (4*c, 3*r)\n",
    "        if ax is None: _,ax = plt.subplots(r, c, figsize=figsize)\n",
    "        for i, channel in enumerate(sg.data):\n",
    "            if r == 1:\n",
    "                cur_ax = ax[i%c]\n",
    "            elif c == 1:\n",
    "                cur_ax = ax[i%r]\n",
    "            else:\n",
    "                cur_ax = ax[i//c,i%c]\n",
    "            cur_ax.set_title(f\"{fname}: Channel {i//c} Image {i%c}\")\n",
    "            z = specshow(channel.numpy(), ax=cur_ax, **sg._show_args, **proper_kwargs)\n",
    "            #plt.colorbar(z, ax=cur_ax)\n",
    "            #ax=plt.gca() #get the current axes\n",
    "            #PCM=ax.get_children()[2] #get the mappable, the 1st and the 2nd are the x and y axes\n",
    "            #plt.colorbar(PCM, ax=ax, format='%+2.0f dB') \n",
    "            \n",
    "def _show_spectrogram(sg, ax, proper_kwargs, **kwargs):\n",
    "    if \"mel\" not in sg.settings: y_axis = None\n",
    "    else:                        y_axis = \"mel\" if sg.mel else \"linear\"\n",
    "    proper_kwargs.update({\"x_axis\":\"time\", \"y_axis\":y_axis,})\n",
    "    _ = specshow(sg.data.squeeze(0).numpy(), **sg._show_args, **proper_kwargs)\n",
    "    fmt = '%+2.0f dB' if \"to_db\" in sg.settings and sg.to_db else '%+2.0f'\n",
    "    plt.colorbar(format=fmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'><strong>Note:</strong><br> _validate and _warn_kwargs should probably be abstracted up a level, they dont belong to AudioToSpec class and could be useful to check args in general.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectrogram Generation: AudioToSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(_GenSpec.__init__)\n",
    "@delegates(_GenMelSpec.__init__, keep=True)\n",
    "@delegates(_ToDB.__init__, keep=True)\n",
    "class AudioToSpec(Transform):\n",
    "    def __init__(self, mel=True, to_db=True, **kwargs):\n",
    "        self._validate_kwargs(mel, to_db, kwargs)\n",
    "        transforms = L()\n",
    "        kwargs = self.add_local_defaults(dict(kwargs))\n",
    "        if mel:   transforms += self.add_func(_GenMelSpec, kwargs)\n",
    "        else:     transforms += self.add_func(_GenSpec, kwargs)\n",
    "        if to_db: transforms += self.add_func(_ToDB, kwargs)\n",
    "        #would it be better to use Pipeline here than nn.Sequential?\n",
    "        self.transformer = nn.Sequential(*transforms)\n",
    "        store_attr(self, 'to_db,mel')\n",
    "        self.__dict__.update(kwargs)\n",
    "        \n",
    "    @classmethod\n",
    "    def from_cfg(cls, audio_cfg):\n",
    "        cfg = asdict(audio_cfg) if is_dataclass(audio_cfg) else audio_cfg\n",
    "        return cls(**cfg)\n",
    "    \n",
    "    def encodes(self, x:AudioItem):\n",
    "        settings = dict(self.__dict__)\n",
    "        settings.update({'sr':x.sr, 'nchannels':x.nchannels, 'path':x.path})\n",
    "        return AudioSpectrogram.create(self.transformer(x.sig).flip(1).detach(), settings=settings)\n",
    "    \n",
    "    def add_func(self, func, kwargs):\n",
    "        func_args = get_usable_kwargs(func, kwargs, [])\n",
    "        self.__dict__.update(func_args)\n",
    "        return func(**func_args)\n",
    "    \n",
    "    # Torchaudio overrides None values internally for these objects, their logic is copied here for now\n",
    "    # so that the settings stored in the spectrogram accurately reflect what is happening.\n",
    "    # Also we override their default n_fft of 400 because it is very bad if n_mels > 64\n",
    "    def add_local_defaults(self, kwargs):\n",
    "        if \"n_fft\" not in kwargs or kwargs[\"n_fft\"] is None:            kwargs[\"n_fft\"] = 1024\n",
    "        if \"win_length\" not in kwargs or kwargs[\"win_length\"] is None:  kwargs[\"win_length\"] = kwargs[\"n_fft\"] \n",
    "        if \"hop_length\" not in kwargs or kwargs[\"hop_length\"] is None:  kwargs[\"hop_length\"] = int(kwargs[\"win_length\"]/2)\n",
    "        return kwargs\n",
    "    \n",
    "    @staticmethod\n",
    "    def _validate_kwargs(mel, to_db, kwargs):\n",
    "        funcs = [_GenMelSpec, _GenSpec, _ToDB]\n",
    "        all_args = set().union(*map(lambda x: set(inspect.signature(x).parameters.keys()), funcs))\n",
    "        for k, v in kwargs.items():\n",
    "            if k not in all_args:\n",
    "                warnings.warn(f\"{k} is not a valid arg name, usable kwargs are {all_args}\")\n",
    "        if mel:       AudioToSpec._warn_kwargs(_GenMelSpec, _GenSpec, kwargs)  \n",
    "        else  :       AudioToSpec._warn_kwargs(_GenSpec, _GenMelSpec, kwargs)\n",
    "        if not to_db: AudioToSpec._warn_kwargs(noop, _ToDB, kwargs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _warn_kwargs(used, unused, kwargs):\n",
    "        def get_bad_args(f1, f2):\n",
    "            a1, a2 = map(lambda x: set(inspect.signature(x).parameters.keys()), (f1, f2))\n",
    "            return a2 - a1\n",
    "        bad_args = get_bad_args(used, unused)\n",
    "        for k, v in kwargs.items():\n",
    "            if(k in bad_args):\n",
    "                warnings.warn(f\"{k} passed in but unused, your settings use {used} not {unused}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sg with weird settings for testing\n",
    "a2s = AudioToSpec(f_max=20000, n_mels=137)\n",
    "sg = a2s(item0)\n",
    "sg2 = a2s(item100)\n",
    "sg_mc = a2s(fake_multichannel)\n",
    "sg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.show()\n",
    "sg2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_mc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.nchannels, sg.height, sg.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the explicit settings were properly stored in the spectrogram object and can be accessed as attributes\n",
    "test_eq(sg.f_max, 20000)\n",
    "test_eq(sg.hop_length, 512)\n",
    "test_eq(sg.sr, item100.sr)\n",
    "test_eq(sg.mel, True)\n",
    "test_eq(sg.to_db, True)\n",
    "test_eq(sg.nchannels, 1)\n",
    "test_eq(sg.height, 137)\n",
    "test_eq(sg.n_mels, sg.height)\n",
    "test_eq(sg.width, 114)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults = {k:v.default for k, v in inspect.signature(_GenMelSpec).parameters.items()}\n",
    "a2s = AudioToSpec(f_max=20000, hop_length = 345)\n",
    "sg = a2s(item100)\n",
    "test_eq(sg.n_mels, defaults[\"n_mels\"])\n",
    "test_eq(sg.n_fft , 1024)\n",
    "test_eq(sg.shape[1], sg.n_mels)\n",
    "test_eq(sg.hop_length, 345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the spectrogram and audio have same duration, both are computed\n",
    "# on the fly as transforms can change their duration\n",
    "test_close(sg.duration, item100.duration, eps=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test warnings for missing/extra arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_W=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test warning for unused argument 'power' for melspec\n",
    "#tests AudioToSpec and its from_cfg class method\n",
    "voice_mel_cfg = {'n_fft':2560, 'f_max':22050., 'n_mels':128, 'hop_length':256, 'power':2}\n",
    "test_warns(lambda: AudioToSpec(**voice_mel_cfg), show=SHOW_W)\n",
    "test_warns(lambda: AudioToSpec.from_cfg(voice_mel_cfg), show=SHOW_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test for unused arguments 'f_max' and 'n_mels' for non-mel Spectrogram\n",
    "voice_mel_cfg = {'f_max':22050., 'n_mels':128, 'n_fft':2560, 'hop_length':256, 'power':2}\n",
    "test_warns(lambda: AudioToSpec(mel=False, **voice_mel_cfg), show=SHOW_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test warning for unused argument 'top_db' when db conversion not done\n",
    "voice_mel_cfg = {'top_db':20, 'n_fft':2560, 'f_max':22050., 'n_mels':128, 'hop_length':256}\n",
    "test_warns(lambda: AudioToSpec(to_db=False, **voice_mel_cfg), show=SHOW_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test warning for invalid argument 'doesntexist'\n",
    "voice_mel_cfg = {'doesntexist':True, 'n_fft':2560, 'f_max':22050., 'n_mels':128, 'hop_length':256}\n",
    "test_warns(lambda: AudioToSpec(to_db=False, **voice_mel_cfg), show=SHOW_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AudioToSpec Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_to_db_mel = AudioToSpec()\n",
    "a_to_nondb_mel = AudioToSpec(to_db=False)\n",
    "a_to_db_nonmel = AudioToSpec(mel=False)\n",
    "a_to_nondb_non_mel = AudioToSpec(mel=False, to_db=False)\n",
    "a_to_db_mel_hyperparams = AudioToSpec(n_fft=8192, hop_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "a_to_db_mel(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "a_to_nondb_mel(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "a_to_db_nonmel(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "a_to_nondb_non_mel(item0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n10\n",
    "# Time can blow up as a factor of n_fft and hop_length. n_fft is best kept to a power of two, hop_length\n",
    "# doesn't matter except smaller = more time because we have more chunks to perform STFTs on\n",
    "a_to_db_mel_hyperparams(item0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AudioToSpec Timing Tests as audio length scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def time_variable_length_audios(f, max_seconds=30, sr=16000, channels=1):\n",
    "    times = []\n",
    "    audios = [AudioItem((torch.randn(channels, sr*i), sr, None)) for i in range(1,max_seconds+1,2)]\n",
    "    for a in audios:\n",
    "        start = time.time()\n",
    "        out = f(a)\n",
    "        end = time.time()\n",
    "        times.append(round(1000*(end-start), 2))\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a2s = AudioToSpec()\n",
    "max_seconds = 180\n",
    "times_mono = time_variable_length_audios(f=a2s, max_seconds=max_seconds)\n",
    "times_stereo = time_variable_length_audios(f=a2s, max_seconds=max_seconds, channels=2)\n",
    "plt.plot(np.arange(0,max_seconds,2), times_mono, label=\"mono\")\n",
    "plt.plot(np.arange(0,max_seconds,2), times_stereo, label=\"stereo\")\n",
    "plt.legend(['mono','stereo'])\n",
    "plt.title(\"Time Taken by AudioToSpec\")\n",
    "plt.xlabel(\"Audio Duration in Seconds\")\n",
    "plt.ylabel(\"Processing Time in ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'><strong>Issue:</strong><br>\n",
    "    MFCC is based on a melspectrogram so it accepts a bunch of the same arguments, but instead of passing them in explicitly, they are passed as a dict to \"melkwargs\". As a result, in the current state the mfcc has no current info about the hop_length (determines the width) that it was generated with. One option is grabbing the defaults from _GenMelSpec inside AudioToMFCC and pass it into the sg_settings. OTOH this could be an argument for lumping everything into AudioToSpec, including MFCC, and then we'd have the same access to _GenMelSpec arguments for tab-completion. We could also make AudioToMFCC have a 2nd delegation to _GenMelSpec, and then parse the MelSpec arguments ourselves and bundle them into melkwargs before passing them to torchaudio. This would break our concept of wrapping the external functions in internal references like _GenMelSpec, because we'd no longer be agnostic to how theyre implemented. One last note is that melkwargs will not accept extra keywords, only the ones that torchaudio.transforms.MelSpectrogram expects. <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(_GenMFCC.__init__)\n",
    "class AudioToMFCC(Transform):\n",
    "    def __init__(self,**kwargs):\n",
    "        func_args = get_usable_kwargs(_GenMFCC, kwargs, [])\n",
    "        self.transformer = _GenMFCC(**func_args)\n",
    "        self.settings = func_args\n",
    "        \n",
    "    @classmethod\n",
    "    def from_cfg(cls, audio_cfg):\n",
    "        cfg = asdict(audio_cfg) if is_dataclass(audio_cfg) else audio_cfg\n",
    "        return cls(**cfg)\n",
    "    \n",
    "    def encodes(self, x:AudioItem):\n",
    "        sg_settings = {\"sr\":x.sr, 'nchannels':x.nchannels,'path':x.path, **self.settings}\n",
    "        return AudioSpectrogram.create(self.transformer(x.sig).detach(), settings=sg_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2mfcc = AudioToMFCC()\n",
    "mfcc = a2mfcc(item0)\n",
    "test_eq(mfcc.n_mfcc, mfcc.data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc.settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc.width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_mfcc specified should determine the height of the mfcc\n",
    "n_mfcc = 67\n",
    "a2mfcc67 = AudioToMFCC(n_mfcc=n_mfcc)\n",
    "mfcc67 = a2mfcc67(item100)\n",
    "test_eq(mfcc67.shape[1], n_mfcc)\n",
    "print(mfcc67.shape)\n",
    "mfcc67.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of passing in melkwargs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2mfcc_kwargs = AudioToMFCC(melkwargs={\"hop_length\":1024, \"n_fft\":1024})\n",
    "mfcc_kwargs = a2mfcc_kwargs(item100)\n",
    "mfcc_kwargs.show()\n",
    "# make sure a new hop_length changes the resulting width\n",
    "test_ne(mfcc_kwargs.width, mfcc.width)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MFCC Timing Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "a2mfcc = AudioToMFCC()\n",
    "max_seconds = 180\n",
    "times_mono = time_variable_length_audios(f=a2mfcc, max_seconds=max_seconds)\n",
    "times_stereo = time_variable_length_audios(f=a2mfcc, max_seconds=max_seconds, channels=2)\n",
    "plt.plot(np.arange(0,max_seconds,2), times_mono, label=\"mono\")\n",
    "plt.plot(np.arange(0,max_seconds,2), times_stereo, label=\"stereo\")\n",
    "plt.legend(['mono','stereo'])\n",
    "plt.title(\"Time Taken by AudioToMFCC\")\n",
    "plt.xlabel(\"Audio Duration in Seconds\")\n",
    "plt.ylabel(\"Processing Time in ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB MelSpectrogram Pipe (Standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_cfg = {'n_fft':2560,'hop_length':64}\n",
    "oa = OpenAudio(files)\n",
    "a2s = AudioToSpec(**mel_cfg)\n",
    "db_mel_pipe = Pipeline([oa,a2s], as_item=True)\n",
    "for i in range(5):\n",
    "    print(\"Shape:\", db_mel_pipe(i).shape)\n",
    "    db_mel_pipe.show(db_mel_pipe(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Spectrogram (non-mel, non-db) Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {'hop_length':128, 'n_fft':400}\n",
    "oa = OpenAudio(files)\n",
    "db_mel_pipe = Pipeline([oa, AudioToSpec(mel=False, to_db=False, **cfg)], as_item=True)\n",
    "for i in range(3):\n",
    "    print(\"Shape:\", db_mel_pipe(i).shape)\n",
    "    db_mel_pipe.show(db_mel_pipe(i))\n",
    "    test_eq(db_mel_pipe(i).hop_length, cfg[\"hop_length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBScale non-melspectrogram Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa = OpenAudio(files)\n",
    "db_mel_pipe = Pipeline([oa, AudioToSpec(mel=False)], as_item=True)\n",
    "for i in range(3): \n",
    "    print(\"Shape:\", db_mel_pipe(i).shape)\n",
    "    db_mel_pipe.show(db_mel_pipe(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe using from_cfg (config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-mel db-scale spectrogram \n",
    "cfg = {'mel':False, 'n_fft':260, 'f_max':22050., 'hop_length':128}\n",
    "oa = OpenAudio(files)\n",
    "db_mel_pipe = Pipeline([oa, AudioToSpec.from_cfg(cfg)], as_item=True)\n",
    "for i in range(3): \n",
    "    db_mel_pipe.show(db_mel_pipe(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_mfcc_pipe = Pipeline([oa, AudioToMFCC(n_mfcc=40),], as_item=True)\n",
    "for i in range(3): \n",
    "    db_mfcc_pipe.show(db_mfcc_pipe(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AudioConfig Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def config_from_func(func, name, **kwargs):\n",
    "    params = inspect.signature(func).parameters.items()\n",
    "    namespace = {k:v.default for k, v in params}\n",
    "    namespace.update(kwargs)\n",
    "    return make_dataclass(name, namespace.keys(), namespace=namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AudioConfig():\n",
    "    #default configurations from the wrapped function\n",
    "    #make sure to pass in mel=False as kwarg for non-mel spec, and to_db=False for non db spec\n",
    "    BasicSpectrogram    = config_from_func(_GenSpec, \"BasicSpectrogram\", mel=False)\n",
    "    BasicMelSpectrogram = config_from_func(_GenMelSpec, \"BasicMelSpectrogram\")\n",
    "    BasicMFCC           = config_from_func(_GenMFCC, \"BasicMFCC \")\n",
    "    #special configs with domain-specific defaults\n",
    "\n",
    "    Voice = config_from_func(_GenMelSpec, \"Voice\", f_min=50., f_max=8000., n_fft=1024, n_mels=128, hop_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Mel Spectrogram is just the Torchaudio defaults, which are currently bad, hence\n",
    "# the empty melbins in the spectrogram below. We can make our own custom good ones like Voice\n",
    "mel_cfg = AudioConfig.BasicMelSpectrogram()\n",
    "a2mel = AudioToSpec.from_cfg(mel_cfg)\n",
    "mel_bad = a2mel(oa(42))\n",
    "mel_bad.show()\n",
    "voice_cfg = AudioConfig.Voice()\n",
    "a2mel = AudioToSpec.from_cfg(voice_cfg)\n",
    "mel_good = a2mel(oa(42))\n",
    "mel_good.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(mel_bad.n_fft, mel_cfg.n_fft)\n",
    "# hop defaults to None in torchaudio but is set later in the code, we override this default to None\n",
    "# internally in AudioToSpec to ensure the correct hop_length is stored as a sg attribute\n",
    "test_ne(mel_bad.hop_length, mel_cfg.hop_length)\n",
    "print(\"MelConfig Default Hop:\", mel_cfg.hop_length)\n",
    "print(\"Resulting Hop:\",mel_bad.hop_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_cfg = AudioConfig.BasicSpectrogram()\n",
    "# make sure mel setting is passed down and is false for normal spectro\n",
    "test_eq(sg_cfg.mel, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab a random file, test that the n_fft are passed successfully via config and stored in sg settings\n",
    "oa = OpenAudio(files)\n",
    "f_num = random.randint(0, len(files))\n",
    "sg_cfg = AudioConfig.BasicSpectrogram(n_fft=2000, hop_length=155)\n",
    "a2sg = AudioToSpec.from_cfg(sg_cfg)\n",
    "sg = a2sg(oa(f_num))\n",
    "test_eq(sg.n_fft, sg_cfg.n_fft)\n",
    "test_eq(sg.width, int(oa(f_num).nsamples/sg_cfg.hop_length)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline examples from Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa = OpenAudio(files)\n",
    "db_mel_pipe = Pipeline([oa, AudioToSpec.from_cfg(sg_cfg)], as_item=True)\n",
    "for i in range(3): \n",
    "    db_mel_pipe.show(db_mel_pipe(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice_config = AudioConfig.Voice(); voice_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa = OpenAudio(files)\n",
    "db_mel_pipe = Pipeline([oa, AudioToSpec.from_cfg(voice_config)], as_item=True)\n",
    "for i in range(3): \n",
    "    db_mel_pipe.show(db_mel_pipe(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_cfg = AudioConfig.BasicMFCC()\n",
    "oa = OpenAudio(files)\n",
    "mfcc_pipe = Pipeline([oa, AudioToMFCC.from_cfg(mfcc_cfg)], as_item=True)\n",
    "for i in range(44,47):\n",
    "    print(\"Shape\", mfcc_pipe(i).shape)\n",
    "    mfcc_pipe(i).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
