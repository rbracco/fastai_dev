{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.basics import *\n",
    "from local.data.all import *\n",
    "from local.vision import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.audio.core import *\n",
    "from local.audio.augment import *\n",
    "from local.vision.learner import *\n",
    "from local.vision.models.xresnet import *\n",
    "from local.metrics import *\n",
    "from local.audio.core import *\n",
    "from local.audio.augment import *\n",
    "from local.vision.models.xresnet import *\n",
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Config()['data_path'] / 'ST-AEDS-20180100_1-OS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(num=10):\n",
    "    return [AudioItem.create(data_path.ls()[i]) for i in range(num)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def time_f(f):\n",
    "    t = time.time()\n",
    "    res = f()\n",
    "    return time.time() - t\n",
    "\n",
    "def plot_gpu_vs_cpu():    \n",
    "    sg_cuda = Spectrogram().cuda()\n",
    "    sg_cpu =  Spectrogram()\n",
    "\n",
    "    def cuda(audios_cuda):\n",
    "        return sg_cuda(audios_cuda)\n",
    "\n",
    "    def cpu(audios):\n",
    "        return sg_cpu(audios)\n",
    "\n",
    "    crop = CropSignal(8000)\n",
    "    times = []\n",
    "    bsr = range(1, 200, 5)\n",
    "    batch_size = list(bsr) \n",
    "    # try:\n",
    "    bsr = progress_bar(bsr)\n",
    "    with torch.no_grad():\n",
    "        for i in bsr:\n",
    "            audios = [crop(a)[0] for a in load_audio(num=i)]\n",
    "            audios = torch.cat(audios)\n",
    "            audios_cuda = audios.clone().cuda()\n",
    "            res = time_f(partial(cuda, audios_cuda)),  time_f(partial(cpu, audios))\n",
    "            times.append(res)\n",
    "            gc.collect()\n",
    "    \n",
    "    gpu_times, cpu_times = list(zip(*times))\n",
    "    plt.plot(batch_size[:len(gpu_times)], gpu_times, label='GPU')\n",
    "    plt.plot(batch_size[:len(gpu_times)], cpu_times, label='CPU')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Seconds')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p10speakers = Config()['data_path'] / 'ST-AEDS-20180100_1-OS'\n",
    "untar_data(URLs.SPEAKERS, dest=p10speakers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AudioBlock(cls=AudioItem): return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor)\n",
    "a2mfcc = AudioToMFCC(n_mffc=20, melkwargs={\"n_fft\":2048, \"hop_length\":256, \"n_mels\":128})\n",
    "auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_items=get_audio_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=lambda x: str(x).split('/')[-1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to Kevin Bird and Hiromi Suenaga for these two lines to adjust a CNN model to take 1 channel input\n",
    "def alter_learner(learn, channels=1):\n",
    "    learn.model[0][0].in_channels=channels\n",
    "    learn.model[0][0].weight = torch.nn.parameter.Parameter(learn.model[0][0].weight[:,1,:,:].unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioToSpecCuda(AudioToSpec):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.transformer = self.transformer.cuda()\n",
    "        \n",
    "    def encodes(self, x:Tensor) -> Tensor:\n",
    "        if x.dim() == 3:\n",
    "            return self.transformer(x[:, -1, :])[:, None].detach()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cfg_voice = AudioConfig.Voice()\n",
    "a2s = AudioToSpec.from_cfg(cfg_voice)\n",
    "crop_2000ms = CropSignal(2000)\n",
    "def tosig(t:AudioItem):return t.sig\n",
    "tfms = Pipeline([crop_2000ms, a2s], as_item=True)\n",
    "ctfms = Pipeline([crop_2000ms, tosig], as_item=True)\n",
    "btfms = Pipeline([AudioToSpecCuda()], as_item=True)\n",
    "cauds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n",
    "                 get_items=get_audio_files, \n",
    "                 splitter=RandomSplitter(),\n",
    "                 get_y=lambda x: str(x).split('/')[-1][:5])\n",
    "\n",
    "def run_learner(bs):\n",
    "    print(\"CPU\")\n",
    "    \n",
    "    dbunch = auds.databunch(p10speakers, item_tfms=tfms, bs=bs)\n",
    "    learn = Learner(dbunch,\n",
    "                xresnet18(),  \n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "    alter_learner(learn)\n",
    "    learn.fit_one_cycle(2)\n",
    "    \n",
    "def run_learner_cuda(bs):\n",
    "    print(\"GPU\")\n",
    "    dbunch = cauds.databunch(p10speakers, item_tfms=ctfms, batch_tfms=btfms, bs=bs)\n",
    "    learn = Learner(dbunch,\n",
    "                xresnet18(),  \n",
    "                torch.nn.CrossEntropyLoss(), \n",
    "                metrics=[accuracy])\n",
    "    alter_learner(learn)\n",
    "    learn.fit_one_cycle(2)\n",
    "    \n",
    "def run_exp():\n",
    "    times = []\n",
    "    bsr = range(64, 200, 5)\n",
    "    batch_size = list(bsr) \n",
    "    # try:\n",
    "    bsr = progress_bar(bsr)\n",
    "    try:\n",
    "        for i in bsr:\n",
    "            res = time_f(partial(run_learner_cuda, i)), time_f(partial(run_learner, i))\n",
    "            times.append(res)\n",
    "            try: 1/0 \n",
    "            except: pass\n",
    "            gc.collect()\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    \n",
    "    gpu_times, cpu_times = list(zip(*times))\n",
    "    plt.plot(batch_size[:len(gpu_times)], gpu_times, label='GPU')\n",
    "    plt.plot(batch_size[:len(gpu_times)], cpu_times, label='CPU')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Batch Size')\n",
    "    plt.ylabel('Seconds')\n",
    "    plt.show()\n",
    "\n",
    "run_exp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask Freq + Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.jit.script\n",
    "def torchrint(max_y:int):\n",
    "    return torch.randint(0,max_y,(1,1)).squeeze()\n",
    "\n",
    "@torch.jit.script\n",
    "def meth(sg_batch:Tensor, size:int=20):\n",
    "    bsg = sg_batch.clone()\n",
    "    max_y = bsg.shape[-2]-size-1\n",
    "    for i in range(bsg.shape[0]):\n",
    "        s = bsg[i, :]\n",
    "        m = s.flatten(-2).mean()\n",
    "        r = torchrint(max_y).item()\n",
    "        s[:, r:r+size, :] = m\n",
    "    return bsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = auds.databunch(p10speakers, item_tfms=tfms, bs=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch = dbunch.one_batch()[0]\n",
    "cpub = batch.clone().cpu()\n",
    "plt.imshow(batch[0][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cpub.transpose(3,2)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = meth(cpub.transpose(-1,-2)).transpose(-1,-2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x[0][0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
